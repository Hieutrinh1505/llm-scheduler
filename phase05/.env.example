# LLM Scheduler Configuration
# Copy this file to .env and customize

# Model to use for inference
# Examples: gpt2, gpt2-medium, gpt2-large, TinyLlama/TinyLlama-1.1B-Chat-v1.0
LLM_MODEL=gpt2

# vLLM server endpoint (for vLLM scheduler benchmarks)
VLLM_ENDPOINT=http://localhost:8000/v1/completions

# vLLM server configuration
VLLM_PORT=8000
VLLM_HOST=0.0.0.0

# GPU memory utilization (0.0 to 1.0)
# Lower values leave more memory for other processes
GPU_MEMORY_UTIL=0.9
